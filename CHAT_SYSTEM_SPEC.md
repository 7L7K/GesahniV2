# Gesahni Chat System - Technical Specification

## Overview

The Gesahni chat system implements a sophisticated multi-layered architecture with skill-based routing, real-time streaming, and persistent conversation history. This document provides the complete technical specification for the chat protocol, routing logic, persistence model, and implementation details.

## 1. SSE Event Contracts

### Event Format

All Server-Sent Events follow this standardized format:

```
data: {"event": "<event_type>", "data": <payload>}
\n\n
```

### Event Types & Payloads

#### `delta` - Token Streaming
Emitted for each token as it's generated by the LLM or skill.

```json
{
  "event": "delta",
  "data": {
    "content": "token_string"
  }
}
```

**Usage**: Real-time text streaming to provide immediate feedback to users.

#### `final` - Successful Completion
Emitted when the response is complete and successful.

```json
{
  "event": "final", 
  "data": {
    "rid": "request_id_8chars",
    "vendor": "openai|ollama|skill",
    "model": "gpt-4o|llama3:latest|TimerSkill",
    "usage": {
      "input_tokens": 42,
      "output_tokens": 1337
    }
  }
}
```

**Usage**: Signals completion with metadata about the backend used and token consumption.

#### `error` - Terminal Failure
Emitted when a request fails and cannot be recovered.

```json
{
  "event": "error",
  "data": {
    "rid": "request_id_8chars",
    "code": "upstream_error|upstream_stall|unknown_model|internal_error",
    "error": "optional descriptive string"
  }
}
```

**Error Codes**:
- `upstream_error`: Backend service failure
- `upstream_stall`: Backend timed out or stalled
- `unknown_model`: Invalid model override specified
- `internal_error`: Unexpected system error

#### `ping` - Connection Health Check
Periodic heartbeat to maintain connection health.

```json
{
  "event": "ping",
  "data": {
    "ts": 1640995200.123
  }
}
```

**Usage**: Sent every 8 seconds during active streaming to detect connection issues.

#### `heartbeat` (Legacy)
Legacy heartbeat event, maintained for backward compatibility but not used in primary streaming.

```json
{
  "event": "heartbeat", 
  "data": {
    "ts": "2024-01-01T12:00:00.000Z"
  }
}
```

## 2. Router Logic Details

### Skill Confidence Scoring Algorithm

The skill selection system uses a multi-factor confidence scoring approach:

```python
def score_skill(skill, match) -> tuple[float, dict[str, float]]:
    groups = match.groupdict() if match else {}
    matched = len([v for v in groups.values() if v])
    total = max(1, len(groups))
    pattern_score = matched / total  # 0.0 to 1.0
    
    context_bonus = 0.0
    # Boost when both amount and unit are present (stronger signal)
    if match and match.groupdict().get("amount") and match.groupdict().get("unit"):
        pattern_score = min(1.0, pattern_score + 0.05)
    
    # Context bonus: skill name appears in prompt
    skill_base = skill.__class__.__name__.lower().replace("skill", "")
    aliases = {skill_base}
    aliases.update({"timer": "timerskill"})  # explicit alias map
    
    prompt_text = (match.string or "").lower()
    for alias in aliases:
        if alias and alias in prompt_text:
            context_bonus += 0.05
            break
    
    recent_success = 0.0  # stub for future ML scoring
    score = min(1.0, pattern_score + context_bonus + recent_success)
    return score, {
        "pattern_score": pattern_score, 
        "context_bonus": context_bonus
    }
```

### Selection Thresholds & Decision Flow

```python
# Skill selection threshold
SKILL_THRESHOLD = 0.6

# Cost estimation (prevents division by zero)
cost = max(0.01, float(skill.cost_estimate(text)))

# Final selection score = confidence / cost
selection_score = confidence / cost

# Selection logic
if selection_score >= SKILL_THRESHOLD:
    return skill.handle(request)
else:
    return route_to_llm(request)
```

### Tie Breaking & Selection Priority

- **Tie breaking**: First skill in registration order (deterministic but arbitrary)
- **Cost factor**: Prevents runaway confidence scores from dominating selection
- **Threshold rationale**: 0.6 provides balance between precision and coverage

### Route Decision Observability

All routing decisions are logged with structured telemetry:

```json
{
  "observability": {
    "route_decision": {
      "skill_won": "TimerSkill|null",
      "intent": "timer_set|general", 
      "model": "gpt-4o|llama3:latest",
      "vendor": "openai|ollama|skill",
      "cache_hit": false,
      "confidence_score": 0.85,
      "cost_estimate": 0.02
    },
    "hooks": {
      "results": [...],
      "ok": true
    }
  }
}
```

### Telemetry Metrics

- `SKILL_CONF_BUCKET`: Histogram of skill confidence scores
- `SKILL_HITS_TOTAL`: Counter of skill usage by skill name
- `SELECTOR_LATENCY_MS`: Performance monitoring for skill selection

## 3. Persistence Model

### Database Schema

#### Table: `chat.chat_messages`

```sql
-- Chat message persistence
CREATE SCHEMA IF NOT EXISTS chat;

CREATE TABLE chat.chat_messages (
    id SERIAL PRIMARY KEY,
    user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
    rid VARCHAR(32) NOT NULL,  -- Request ID (8-char prefix)
    role VARCHAR(20) NOT NULL, -- system|user|assistant
    content TEXT NOT NULL,
    created_at TIMESTAMPTZ NOT NULL DEFAULT now()
);

-- Performance indexes
CREATE INDEX ix_chat_chat_messages_rid ON chat.chat_messages(rid);
CREATE INDEX ix_chat_messages_user_rid ON chat.chat_messages(user_id, rid);
CREATE INDEX ix_chat_messages_created_at ON chat.chat_messages(created_at);
```

### Key Fields

| Field | Type | Description |
|-------|------|-------------|
| `id` | SERIAL | Auto-incrementing primary key |
| `user_id` | UUID | Foreign key to auth.users (with CASCADE delete) |
| `rid` | VARCHAR(32) | Request ID for conversation grouping |
| `role` | VARCHAR(20) | Message type: system/user/assistant |
| `content` | TEXT | Full message content |
| `created_at` | TIMESTAMPTZ | Auto-timestamped creation time |

### Conversation Stitching Logic

#### Query Pattern

```sql
-- Retrieve complete conversation by request ID
SELECT id, role, content, created_at 
FROM chat.chat_messages 
WHERE user_id = $1 AND rid = $2 
ORDER BY created_at ASC;
```

#### Stitching Rules

1. **RID-based grouping**: All messages with same `rid` belong to one conversation
2. **Temporal ordering**: Messages ordered by `created_at` within RID
3. **User isolation**: Strict `user_id` filtering for privacy
4. **No session boundaries**: Conversations can span multiple sessions via RID
5. **Message types**: system (context), user (input), assistant (response)

#### Replay API Response Format

```json
{
  "rid": "abc12345",
  "user_id": "uuid-here",
  "message_count": 3,
  "messages": [
    {
      "id": 123,
      "role": "user",
      "content": "Set a timer for 5 minutes",
      "created_at": "2024-01-01T12:00:00.000Z"
    },
    {
      "id": 124,
      "role": "assistant", 
      "content": "Timer set for 5 minutes",
      "created_at": "2024-01-01T12:00:01.000Z"
    }
  ]
}
```

## 4. Frontend Stream Reader Implementation

### Primary Implementation: ReadableStream API

```typescript
async function sendPrompt(prompt: string, modelOverride: string, onToken?: (token: string) => void): Promise<string> {
  const headers: HeadersInit = { Accept: "text/event-stream" };
  const payload: Record<string, unknown> = { prompt };
  if (modelOverride && modelOverride !== "auto") payload.model_override = modelOverride;

  const res = await apiFetch("/v1/ask", { method: "POST", headers, body: JSON.stringify(payload) });
  
  // Content-Type negotiation
  const contentType = res.headers.get("content-type") || "";
  const isSse = contentType.includes("text/event-stream");
  
  if (!res.ok) {
    throw new Error(`Request failed: ${res.status}`);
  }

  // Primary streaming path (browsers)
  const bodyStream = (res as any).body;
  if (bodyStream && typeof bodyStream.getReader === 'function') {
    return await processReadableStream(bodyStream, isSse, onToken);
  }
  
  // Fallback for jsdom/test environments
  return await processTextFallback(res, isSse, onToken);
}

async function processReadableStream(
  bodyStream: ReadableStream<Uint8Array>, 
  isSse: boolean, 
  onToken?: (token: string) => void
): Promise<string> {
  const reader = bodyStream.getReader();
  const decoder = new TextDecoder();
  
  let buffer = "";
  let result = "";
  
  try {
    while (true) {
      const { value, done } = await reader.read();
      if (done) break;
      
      const chunk = decoder.decode(value, { stream: true });
      buffer += chunk;
      
      // Process complete SSE events from buffer
      let idx;
      while ((idx = buffer.indexOf("\\n\\n")) !== -1) {
        const event = buffer.slice(0, idx);
        buffer = buffer.slice(idx + 2);
        
        if (isSse) {
          result += processSseEvent(event, onToken);
        }
      }
      
      // Handle non-SSE streaming
      if (!isSse && buffer) {
        result += processRawChunk(buffer, onToken);
        buffer = "";
      }
    }
  } finally {
    reader.releaseLock();
  }
  
  return result;
}

function processSseEvent(event: string, onToken?: (token: string) => void): string {
  let result = "";
  
  for (const line of event.split("\\n")) {
    if (line.startsWith("data: ")) {
      const data = line.slice(6);
      
      // Handle error events
      if (data.startsWith("[error")) {
        const msg = data.replace(/\\[error:?|\\]$/g, "").trim();
        throw new Error(msg);
      }
      
      // Handle JSON error objects
      if (data.trim().startsWith("{")) {
        try {
          const errorObj = JSON.parse(data.trim());
          if (errorObj.error) {
            throw new Error(errorObj.detail || errorObj.error);
          }
        } catch (parseError) {
          // If JSON parsing fails, treat as normal data
          console.warn("Failed to parse error JSON:", parseError);
        }
      }
      
      // Process successful data
      result += data;
      onToken?.(data);
    }
  }
  
  return result;
}
```

### Fallback Implementation (Tests/JSDOM)

```typescript
async function processTextFallback(
  res: Response, 
  isSse: boolean, 
  onToken?: (token: string) => void
): Promise<string> {
  const text = await res.text();
  let result = "";
  
  if (isSse || text.includes("data:")) {
    // Parse SSE format from complete response
    for (const event of text.split("\\n\\n")) {
      for (const line of event.split("\\n")) {
        if (line.startsWith("data: ")) {
          const data = line.slice(6);
          
          // Same error handling as primary path
          if (data.startsWith("[error")) {
            const msg = data.replace(/\\[error:?|\\]$/g, "").trim();
            throw new Error(msg);
          }
          
          if (data.trim().startsWith("{")) {
            try {
              const errorObj = JSON.parse(data.trim());
              if (errorObj.error) {
                throw new Error(errorObj.detail || errorObj.error);
              }
            } catch (parseError) {
              console.warn("Failed to parse error JSON:", parseError);
            }
          }
          
          result += data;
          onToken?.(data);
        }
      }
    }
  } else {
    // Raw text response
    onToken?.(text);
    return text;
  }
  
  return result;
}
```

### Error Handling & Connection Management

```typescript
// Connection abort/cancellation
const abortController = new AbortController();
const timeoutId = setTimeout(() => abortController.abort(), 60000); // 60s total

try {
  const res = await fetch("/v1/ask", {
    method: "POST",
    headers: { "Accept": "text/event-stream" },
    body: JSON.stringify(payload),
    signal: abortController.signal
  });
  
  // Process stream...
  
} catch (error) {
  if (error.name === 'AbortError') {
    console.log('Request cancelled');
  } else {
    throw error;
  }
} finally {
  clearTimeout(timeoutId);
}
```

## 5. SkillKit Architecture

### Skill Base Class

```python
class Skill(ABC):
    """Abstract base class for all built-in skills."""
    
    PATTERNS: list[Pattern[str]] = []
    skill_why: str | None = None
    
    def match(self, prompt: str) -> re.Match | None:
        """Test if skill matches the prompt."""
        for pat in self.PATTERNS:
            m = pat.search(prompt)
            if m:
                return m
        return None
    
    @abstractmethod
    async def run(self, prompt: str, match: re.Match) -> str:
        """Execute the skill and return response text."""
        raise NotImplementedError
    
    async def handle(self, prompt: str) -> str:
        """Convenience wrapper used by router."""
        m = self.match(prompt)
        if not m:
            raise ValueError("no pattern matched")
        
        resp = await self.run(prompt, m)
        return resp
```

### TimerSkill Implementation

#### Match Patterns

```python
PATTERNS = [
    # Pattern 1: "start/set/create timer for N unit" (name optional)
    re.compile(
        r"\\b(?:start|set|begin|create) (?:(?P<name>[\\w\\-]+) )?timer for (?P<amount>\\d+) (?P<unit>seconds?|second|minutes?|minute|mins?|hrs?|hours?)\\b",
        re.I,
    ),
    # Pattern 2: "create a timer for N unit named name"
    re.compile(
        r"\\b(?:start|set|begin|create) (?:a )?timer for (?P<amount>\\d+) (?P<unit>seconds?|second|minutes?|minute|mins?|hrs?|hours?)(?: (?:named|called) (?P<name>[\\w\\-]+))?\\b",
        re.I,
    ),
    # Pattern 3: "set a timer named name for N unit"
    re.compile(
        r"\\b(?:start|set|begin|create) (?:a )?timer (?:named|called) (?P<name>[\\w\\-]+) for (?P<amount>\\d+) (?P<unit>seconds?|second|minutes?|minute|mins?|hrs?|hours?)\\b",
        re.I,
    ),
    # Pattern 4: "pause/resume/cancel timer"
    re.compile(
        r"\\b(?:pause|resume|cancel|stop) (?:(?P<cname>[\\w\\-]+) )?timers?\\b", re.I
    ),
    # Pattern 5: "how long left on timer"
    re.compile(
        r"\\bhow (?:much |long )?left (?:on|for) (?:(?P<qname>[\\w\\-]+) )?timers?\\b",
        re.I,
    ),
]
```

#### Execution Logic

```python
async def run(self, prompt: str, match: re.Match) -> str:
    groups = match.groupdict()
    
    # Name normalization
    def _clean_name(n: str | None) -> str:
        if not n:
            return "gesahni"
        s = n.strip().lower()
        s = re.sub(r"^(the|my|a|an)\\s+", "", s)  # Remove stopwords
        if s.endswith("s"):
            s = s[:-1]  # Remove plural
        s = re.sub(r"\\s+", "_", s)  # Spaces to underscores
        return s or "gesahni"
    
    # Control actions (pause/resume/cancel)
    if "cname" in groups and groups["cname"] is not None:
        name = _clean_name(groups["cname"])
        action = match.group(0).split()[0].lower()
        
        if action == "pause":
            await ha.call_service("timer", "pause", {"entity_id": f"timer.{name}"})
            return f"{name} timer paused."
        elif action == "resume":
            await ha.call_service("timer", "start", {"entity_id": f"timer.{name}"})
            return f"{name} timer resumed."
        elif action in ("stop", "cancel"):
            await ha.call_service("timer", "cancel", {"entity_id": f"timer.{name}"})
            TIMERS.pop(name, None)
            _persist_timers()
            return f"{name} timer cancelled."
    
    # Query remaining time
    if "qname" in groups:
        name = _clean_name(groups.get("qname"))
        try:
            state = await ha.get_state(f"timer.{name}")
            if state and state.get("state") == "active":
                remaining = state.get("attributes", {}).get("remaining", "unknown")
                return f"{remaining} remaining on {name} timer."
            else:
                return f"{name} timer is not active."
        except Exception:
            return f"Could not check {name} timer status."
    
    # Create new timer
    if "amount" in groups and "unit" in groups:
        amount = int(groups["amount"])
        unit = groups["unit"].lower()
        
        # Unit conversion
        if unit in ("second", "seconds"):
            duration = timedelta(seconds=amount)
        elif unit in ("minute", "minutes", "min", "mins"):
            duration = timedelta(minutes=amount)
        elif unit in ("hour", "hours", "hr", "hrs"):
            duration = timedelta(hours=amount)
        else:
            return "Invalid time unit. Use seconds, minutes, or hours."
        
        name = _clean_name(groups.get("name"))
        
        # Create via Home Assistant
        await ha.call_service("timer", "start", {
            "entity_id": f"timer.{name}",
            "duration": str(duration)
        })
        
        # Persist locally as backup
        end_time = time.time() + duration.total_seconds()
        TIMERS[name] = end_time
        _persist_timers()
        
        return f"Timer '{name}' set for {amount} {unit}."
    
    return "Could not parse timer command."
```

#### Failure Paths

- **Invalid duration**: "Could not parse duration"
- **HA unavailable**: "Failed to control timer"
- **Entity not found**: "Timer not found"
- **Permission denied**: "Access denied to timer control"

### MathSkill Implementation

#### Match Patterns

```python
PATTERNS = [
    # Arithmetic: "3 + 4", "5 * 2", "10 / 2", "8 - 3"
    re.compile(
        r"(?P<a>\\d+(?:\\.\\d+)?)\\s*(?P<op>[+\\-*/x×])\\s*(?P<b>\\d+(?:\\.\\d+)?)", re.I
    ),
    # Percentage: "25% of 100"
    re.compile(r"(?P<pct>\\d+(?:\\.\\d+)?)%\\s*of\\s*(?P<of>\\d+(?:\\.\\d+)?)", re.I),
    # Rounding: "round 3.14159 to 2 decimal"
    re.compile(
        r"round\\s+(?P<val>\\d+(?:\\.\\d+)?)\\s+to\\s+(?P<places>\\d+)\\s+decimal", re.I
    ),
]
```

#### Execution with Fallback Logic

```python
async def run(self, prompt: str, match: re.Match) -> str:
    # First try safe AST evaluation
    expr = prompt.strip()
    try:
        val, expl = evaluate_expr(expr)
    except EvalError:
        # Fall back to regex patterns
        d = match.groupdict()
        if "op" in d and d["op"]:
            try:
                a = float(d["a"])
                b = float(d["b"])
                op = d["op"].lower()
                if op in {"x", "×", "*"}:
                    res = a * b
                elif op == "/":
                    if b == 0:
                        return "Cannot divide by zero"
                    res = a / b
                elif op == "+":
                    res = a + b
                else:
                    res = a - b
                if res.is_integer():
                    res = int(res)
                return str(res)
            except (ValueError, ZeroDivisionError) as e:
                return f"Math error: {e}"
        
        # Other pattern fallbacks...
        if "pct" in d and d["pct"]:
            pct = float(d["pct"])
            of = float(d["of"])
            res = of * (pct / 100.0)
            return str(round(res, 2))
        
        return "Could not compute"
    
    # Format safe AST results
    rec = log_record_var.get()
    if rec is not None:
        rec.route_reason = (rec.route_reason or "") + "|safe_math"
    
    if isinstance(val, bool):
        detail = expl.split(":", 1)[-1].strip()
        if detail.startswith("abs("):
            return f"{val} — because numbers differ by <= {EPS}"
        return f"{val} — because {detail}"
    
    if isinstance(val, float) and val.is_integer():
        val = int(val)
    return f"{val} — {expl}"

async def handle(self, prompt: str) -> str:
    # Try regex patterns first for backward compatibility
    for pat in self.PATTERNS:
        match = pat.search(prompt)
        if match:
            try:
                return await self.run(prompt, match)
            except Exception:
                return "I couldn't parse that as math. Try like: 3*3=9, 2^3, sqrt(16)."
    
    # Fall back to AST evaluation on whole prompt
    try:
        val, expl = evaluate_expr(prompt)
    except EvalError:
        return "I couldn't parse that as math. Try like: 3*3=9, 2^3, sqrt(16)."
    
    if isinstance(val, bool):
        return f"{val} — because {expl.split(':',1)[-1].strip()}"
    if isinstance(val, float) and val.is_integer():
        val = int(val)
    return f"{val} — {expl}"
```

#### Failure Paths & Error Handling

1. **AST evaluation fails** → Try regex patterns
2. **Regex patterns fail** → Return error message, don't route to LLM
3. **Division by zero** → "Cannot divide by zero"
4. **Invalid syntax** → "I couldn't parse that as math. Try like: 3*3=9, 2^3, sqrt(16)."
5. **Overflow/underflow** → AST evaluator handles gracefully

**Key Design Decision**: On parse failure, returns error message instead of routing to LLM. This prevents infinite loops where malformed math would bounce between skill and LLM.

## 6. API Endpoints

### POST `/v1/ask`

Primary chat endpoint supporting both streaming and non-streaming modes.

**Request Format**:
```json
{
  "prompt": "user message text",
  "model": "auto|gpt-4o|llama3:latest",
  "stream": true
}
```

**Response Modes**:
- **Streaming (SSE)**: `text/event-stream` with real-time tokens
- **Non-streaming (JSON)**: `application/json` with complete response

**Authentication**: Bearer token with `chat:write` scope required.

### GET `/v1/ask/replay/{rid}`

Retrieve persisted conversation by request ID.

**Response Format**:
```json
{
  "rid": "abc12345",
  "user_id": "uuid-here",
  "message_count": 3,
  "messages": [
    {
      "id": 123,
      "role": "user",
      "content": "Hello",
      "created_at": "2024-01-01T12:00:00.000Z"
    },
    {
      "id": 124,
      "role": "assistant",
      "content": "Hi there!",
      "created_at": "2024-01-01T12:00:01.000Z"
    }
  ]
}
```

## 7. Configuration & Timeouts

### Backend Timeouts

```python
# Timeout configuration (in milliseconds)
OLLAMA_TIMEOUT_MS = 4500   # 4.5 seconds for Llama
OPENAI_TIMEOUT_MS = 6000   # 6 seconds for OpenAI
STREAM_STALL_MS = 15000    # 15 seconds stall detection
```

### Stream Configuration

```python
# Heartbeat intervals
HEARTBEAT_INTERVAL_S = 30
PING_INTERVAL_S = 8

# Stall detection
STALL_THRESHOLD_S = 15

# Request ID generation
RID_LENGTH = 8  # characters
```

## 8. Error Handling & Resilience

### Circuit Breaker Pattern

- **Llama circuit breaker**: Global fallback when Llama repeatedly fails
- **User circuit breaker**: Per-user fallback after repeated failures
- **Automatic recovery**: Circuit opens after 3 failures, closes after 30 seconds

### Fallback Chain

1. **Primary backend** (chosen by router)
2. **Automatic fallback** (OpenAI ↔ Llama swap)
3. **Error with metadata** (includes failure reason)

### Monitoring & Observability

- **Metrics**: Prometheus counters/histograms for latency, errors, usage
- **Logging**: Structured logs with request IDs and trace correlation
- **Health checks**: Backend availability monitoring

## 9. Testing Strategy

### Unit Tests

- **Skill matching**: Pattern validation with edge cases
- **Router logic**: Confidence scoring and threshold testing
- **Stream parsing**: SSE event processing and error handling

### Integration Tests

- **End-to-end flows**: Complete request → response cycles
- **Fallback testing**: Backend failure simulation
- **Persistence**: Message storage and retrieval

### Load Testing

- **Concurrent streams**: Multiple simultaneous conversations
- **Memory usage**: Long conversation history
- **Timeout handling**: Slow backend simulation

---

*This specification is the authoritative source for the Gesahni chat system implementation. All changes must maintain backward compatibility with existing event contracts and API behavior.*
